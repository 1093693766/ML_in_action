降维数据：
（1）使得数据集更易使用
（2）降低很多算法的计算开销
（3）去除噪声
（4）使得结果易懂




PCA（Principal Component Analysis）主成分分析法：
数据从原来的坐标系转换到了新的坐标系，新坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向；
第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。
发现，大部分方差都包含在最前面的几个新坐标轴中，因此我们可以忽略余下的坐标轴，即对数据进行了降维处理。


因子分析（Factor Analysis）：
在因子分析中我们假设在观察数据的生成中有一些观察不到的隐变量（latent variable）。假设观察数据是这些隐变量和某些噪声的线性组合，
那么隐变量的数目可能比观察数据数目少，即通过隐变量可以实现数据的降维。


独立成分分析（Independent Component Analysis， ICA）：假设数据是从N个数据源生成的，
假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在PCA中只假设数据是不相关的。
同FA一样，如果数据源数目少于观察数据数目，则可以实现降维过程。



PCA：
优点：降低数据的复杂性，识别最重要的多个特征
缺点：不一定需要，且可能损失有用信息
适用数据类型：数值型数据

数据的最大方差给出了数据最重要的信息

Numpy中寻找特征向量和特征值的模块linalg，它有eig()方法，该方法用于求解特征向量和特征值。

Numpy中实现PCA：
去除平均值
计算协方差矩阵
计算协方差矩阵的特征值和特征向量
将特征值从大到小排序
保留最上面的N个特征向量
将数据转换到上述N各特征向量构建的新空间中。



缺失值处理：
使用可用特征的均值来填补缺失值
使用特殊值来填补缺失值，如-1
忽略有缺失值的样本
使用相似样本的均值填补缺失值
使用另外的机器学习算法预测缺失值



数据（data）和信息（information）之间具有巨大差别：
数据指的是接受的原始材料，其中可能包含噪声和不相关的信息。
信息是指数据中的相关部分。

我们可以定量的计算数据中所包含的信息并决定保留的比例。
