SVD矩阵分解
奇异值分解（Singular Value Decomposition, SVD）：

优点：简化数据，去除噪声，提高算法结果
缺点：数据的转换可能难以理解
适用数据类型：数值型数据

从数据中抽取信息：我们可以把SVD看成是从有噪声的数据中抽取相关的特征。

SVD通过隐性语义索引应用于搜索和信息检索领域。最早的SVD应用之一是信息检索，
我们称利用SVD的方法为隐性语义索引（Latent Semantic Indexing， LSI)或隐性语义分析（Latent Semantic Analysis， LSA）

在LSI中，一个矩阵由文档和词语组成。当我们在该矩阵上应用SVD时，就会构建出多个奇异值，
这些奇异值代表了文档中的 概念或主题，这一特点可以用于跟高效的文档搜索



SVD的另一个应用是推荐系统，简单的推荐系统能够计算项或者人之间的相似度，
更先进的方法则是先利用SVD从数据中构建一个主题空间，然后在该空间下计算其相似度。

SVD是矩阵分解的一种类型，而矩阵分解是将数据矩阵分解为多个独立部分的过程。

SVD将原始的数据集矩阵Data分解为三个矩阵U、C、V^T
Data(m*n) = U(m*m)C(m*n)V^T(n*n)     ()里表示矩阵维度

矩阵C，该矩阵只有对角元素，其他元素全为0，其对角元素是从大到小排列的，这些对角元素称为奇异值，他们对应了原始数据集矩阵中的奇异值。

PCA，我们得到的是矩阵的特征值，它告诉我们数据集中的重要特征。


奇异值与特征值是有关系的，这里的奇异值就是Data*Data.T特征值的平方根。

在科学与工程中，在某个奇异值的数目（r个）之后，其他奇异值都置为0，这就意味着数据集中仅有r个重要特征，而其余特征都是噪声或者冗余特征。


Numpy中linalg是线性代数工具箱


基于协同过滤的推荐引擎：
协同过滤是通过将用户和其他用户的数据进行对比来实现推荐的。

相似度计算：
我们不利用专家所给出的重要属性来描述物品从而计算它们之间的相似度，而是利用用户对它们的意见来计算相似度，这就是协同过滤中所使用的方法。
它并不关心物品的描述属性，而是严格的按照许多用户的观点来计算相似度。

（1）欧式距离
    希望相似度值在0到1之间变化，并且物品越相似，它们的相似度值也就越大，可以用“相似度 = 1 / (1+距离)”
    当距离为0时，相似度为1.0，当距离非常大时，相似度也越趋近于0

（2）皮尔逊相关系数（Pearson correlation）
    它度量的是两个向量之间的相似度，优势在于：它对用户评级的量级并不敏感。在Numpy中，皮尔逊相关系数的计算是由函数corrcoef（）进行的。
    其取值范围是-1到+1，可以通过0.5+0.5*corrcoef()计算，并把取值范围归一化到0和1之间。

（3）余弦相似度（cosine similarity）
    其计算的是两个向量夹角的余弦值。如果夹角为90度，则相似度为0；如果两个向量的方向相同，则相似度为1.0。余弦相似度的取值范围也在-1到1之间，
    因此我们也将它归一到0到1之间。
    cos0 = A*B/(||A||*||B||)
    ||A||表示向量A的2范数，可以定义向量的任一范数，但是如果不指定范数阶数，则都假设为2范数，向量[4, 2, 2]的2范数为sqrt（4^2+2^2+2^2）
    Numpy的线性代数工具包中提供了范数的计算方法linalg.norm()

RMSE(Root Mean Squared Error)最小均方根误差：首先计算均方误差的平均值然后取其平方根。

推荐未尝过的菜肴：
（1）寻找用户没有评级的菜肴，即在用户-物品矩阵中的0值
（2）在用户没有评级的所有菜品中，对每个物品预计一个可能的评级分数，即我们认为用户可能会对物品的打分（这就是相似度计算的初衷）
（3）对这些物品的评分从高到低进行排序，返回前N个物品。


