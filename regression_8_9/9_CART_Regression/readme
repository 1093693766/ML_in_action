树回归
CART（Classification And Regression Tree, 分类回归树）

决策树不断将数据切分成小数据集，直到所有的目标变量完全相同，或者数据不能再切分为止。
决策树是一种贪心算法，它要在给定的时间内做出最佳选择，但不关心是否达到全局最优。


树回归：
优点：可以对复杂和非线性的数据建模
缺点：结果不易理解
适用数据类型：数值型和标称型数据

第3章 ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有取值进行切分，
一旦按某种特征进行切分后，该特征在之后的算法执行过程中就不在起作用，
这种切分方式过于迅速且不能处理连续型特征，只有事先将连续型特征转换为离散型特征，才能在ID3算法中使用，但是这种转换会破坏连续型变量的内在性质。



二元切分法：每次把数据集切分成两份，如果数据的某特征值大于切分所要求的值，那么这些数据进入树的左子树，反之则进入树的右子树。二元切分发节省了树的构建时间。


CART使用二元切分法来处理连续型变量，对CART稍作修改就可以处理回归问题。


树回归的一般方法：
（1）收集数据：任意方法
（2）准备数据：需要数值型数据，标称型数据应该映射成二值型数据
（3）分析数据：绘出数据的二维可视化显示结果，以字典方式生成树
（4）训练算法：大部分时间都花在叶节点树模型的构建上
（5）测试算法：使用测试数据上的R^2值来分析模型的效果
（6）使用算法：使用训练出的树做预测，预测结果可以用来做许多事情



用字典来存储树的数据结构，该字典包含以下4元素：
待切分的特征
待切分的特征值
右子树，当不再需要切分的时候，也可以是单个值
左子树，与右子树类似


第三章用一部字典来存储每个切分，该字典可以包含两个或两个以上的值。
而CART算法只做二元切分，所以这里可以固定树的数据结构。
树包含左键和右键，可以存储另一棵子树或者单个值；字典还包含特征和特征值这两个键，它们给出切分算法所有的特征和特征值。

回归树：其每个节点包含单个值
模型树：其每个节点包含一个线性方程


函数createTree（）的伪代码：
找到最佳切分特征：
    如果该节点不能再分，将该节点存为叶节点
    执行二元切分
    在右子树调用createTree（）方法
    在左子树调用createTree（）方法


模型树：把叶节点设定为分段线性函数，模型书的可解释性是其优于回归树的特点之一
误差计算：对于给定数据集，应先用线性模型来对它进行拟合，然后计算真实目标值和模型预测值之间的差值，最后将这些差值的平方求和就得到了所需的误差。

利用GUI对回归树调优“
（1）收集数据： 所提供的文本文件
（2）准备数据：用python解析上述文件，得到数值型数据
（3）分析数据：用Tkinter构建一个GUI来展示模型和数据
（4）训练算法：训练一颗回归树和模型树，并与数据集一起展示出来
（5）测试算法：不需要测试过程
（6）使用算法：GUI使得人们可以在预剪枝时测试不同参数的影响，还可以帮助我们选择模型的类型
