AdaBoost元算法

元算法（meta-algorithm）或者集成方法（ensemble method）是对其他算法进行组合的一种方式


优点：范化错误率低，易编码，可以应用在大部分分类器上，无参数调整
缺点：对离群点敏感
适用数据类型： 数值型和标称型数据

bagging和boosting所使用的多个分类器类型都是一致的！

bagging: 基于数据随机抽样的分类器构建方法
         放回取样得到S个数据集，将某个学习算法分别作用于每个数据集就得到了S个分类器，要对新数据进行分类时，可以应用这S个分类器进行分类，
         选取分类器投票结果中最多的类别作为最后的分类结果。分类器的权重是相等的。

random forest 也是bagging方法的一种


boosting：通过集中关注被已有分类器错分的那些数据来获得新的分类器，
          不同的分类器是通过串行训练获得的，每个新的分类器都根据已训练出的分类器的性能来训练。
          其分类结果是基于所有分类器的加权求和结果的，分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。

AdaBoost一般流程：
（1）收集数据：可以使用任何方法
（2）准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。当然也可以使用任意分类器作为弱分类器，
             2-6章的任一分类器都可以充当弱分类器。作为弱分类器，简单分类器的效果最好。 确保类别标签是+1和-1，而非1和0.
（3）分析数据：可以使用任一方法
（4）训练算法：AdaBoost的大部分时间都用在训练上， 分类器将多次在同一数据集上训练弱分类器。
（5）测试算法：计算分类的错误率
（6）使用算法：同SVM同，AdaBoost预测两个类别中的一个。若想把它应用到多分类的场合，需要像多类SVM中的做法一样对AdaBoost进行修改。

基于代价函数的分类器决策控制：
cost = TP*0 + FP*1 + TN*0 + FN*1
cost = TP*(-5) + FP*50 + TN*0 + FN*1

处理非均衡问题的数据抽样方法：
过抽样或者欠抽样
过抽样是复制样例，欠抽样是删除样例，选择离决策边界较远的样例进行删除。